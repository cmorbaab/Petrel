{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['SNORKELDB'] = 'postgres:///snorkel'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "\n",
    "session = SnorkelSession()\n",
    "from snorkel.parser import XMLMultiDocPreprocessor\n",
    "\n",
    "# The following line is for testing only. Feel free to ignore it.\n",
    "\n",
    "file_path = 'articles/TROP2.xml'\n",
    "TROP2_preprocessor = XMLMultiDocPreprocessor(\n",
    "    path=file_path,\n",
    "    doc='.//Article',\n",
    "    text='./text/text()',\n",
    "    id='./article-id/text()'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel.parser import CorpusParser\n",
    "\n",
    "corpus_parser = CorpusParser()\n",
    "corpus_parser.apply(list(TROP2_preprocessor), parallelism=4) #parallelism can be run with a Postgres DBMS, but not SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves.cPickle import load\n",
    "from snorkel.models import Document, Sentence\n",
    "import cPickle\n",
    "\n",
    "with open('articles/doc_ids.pkl', 'rb') as f:\n",
    "    train_ids, dev_ids, test_ids = load(f)\n",
    "train_ids, dev_ids, test_ids = set(train_ids), set(dev_ids), set(test_ids)\n",
    "print len(train_ids)\n",
    "print len(dev_ids)\n",
    "print len(test_ids)\n",
    "train_sents, dev_sents, test_sents = set(), set(), set()\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if doc.name in train_ids:\n",
    "            train_sents.add(s)\n",
    "        elif doc.name in dev_ids:\n",
    "            dev_sents.add(s)\n",
    "        elif doc.name in test_ids:\n",
    "            test_sents.add(s)\n",
    "        else:\n",
    "            raise Exception('ID <{0}> not found in any id set'.format(doc.name))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASDFASDF\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import Candidate, candidate_subclass\n",
    "\n",
    "BiomarkerCondition = candidate_subclass('BiomarkerCondition', ['biomarker', 'condition'])\n",
    "BiomarkerType = candidate_subclass('BiomarkerType', ['biomarker', 'typ3'])\n",
    "BiomarkerMedium = candidate_subclass('BiomarkerMedium', ['biomarker', 'medium'])\n",
    "BiomarkerLevelUnit = candidate_subclass('BiomarkerLevelUnit', ['biomarker', 'level', 'unit'])\n",
    "BiomarkerDrug = candidate_subclass('BiomarkerDrug', ['biomarker', 'drug'])\n",
    "\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import PersonMatcher\n",
    "import matchers\n",
    "from snorkel.models import Document\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "import os\n",
    "\n",
    "biomarker_ngrams = Ngrams(n_max=1)\n",
    "condition_ngrams = Ngrams(n_max=7)\n",
    "type_ngrams = Ngrams(n_max=5)\n",
    "medium_ngrams = Ngrams(n_max=5)\n",
    "level_ngrams = Ngrams(n_max=8)\n",
    "unit_ngrams = Ngrams(n_max=8)\n",
    "drug_ngrams = Ngrams(n_max=5)\n",
    "\n",
    "# Create our two Matchers\n",
    "bMatcher = matchers.getBiomarkerMatcher()\n",
    "cMatcher = matchers.getDiseaseMatcher()\n",
    "tMatcher = matchers.getTypeMatcher()\n",
    "mMatcher = matchers.getMediumMatcher()\n",
    "lMatcher = matchers.getLevelsMatcher()\n",
    "uMatcher = matchers.getUnitsMatcher()\n",
    "dMatcher = matchers.getDrugMatcher()\n",
    "\n",
    "\n",
    "# Building the CandidateExtractor \n",
    "bc_candidate_extractor = CandidateExtractor(BiomarkerCondition, [biomarker_ngrams, condition_ngrams], [bMatcher, cMatcher])\n",
    "bt_candidate_extractor = CandidateExtractor(BiomarkerType, [biomarker_ngrams, type_ngrams], [bMatcher, tMatcher])\n",
    "bm_candidate_extractor = CandidateExtractor(BiomarkerMedium, [biomarker_ngrams, medium_ngrams], [bMatcher, mMatcher])\n",
    "blu_candidate_extractor = CandidateExtractor(BiomarkerLevelUnit, [biomarker_ngrams, level_ngrams, unit_ngrams], [bMatcher, lMatcher, uMatcher])\n",
    "bd_candidate_extractor = CandidateExtractor(BiomarkerDrug, [biomarker_ngrams, drug_ngrams], [bMatcher, dMatcher])\n",
    "\n",
    "extractors = [bc_candidate_extractor, bt_candidate_extractor, bm_candidate_extractor, blu_candidate_extractor, bd_candidate_extractor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running UDF...\n",
      "[========================================] 100%%\n",
      "\n",
      "Running UDF...\n",
      "[===========                             ] 27%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%RIP\n",
      "Running UDF...\n",
      "[=                                       ] 0%"
     ]
    }
   ],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "sents = []\n",
    "for doc in docs:\n",
    "    for s in doc.sentences:\n",
    "        sents.append(s)\n",
    "for extractor in extractors:\n",
    "    hasCompleted = False\n",
    "    while(not hasCompleted):\n",
    "        try:\n",
    "            extractor.apply(sents, split=0, clear=False)\n",
    "            session.commit()\n",
    "            hasCompleted = True\n",
    "        except:\n",
    "            print \"RIP\"\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import candidate_adjective_fixer\n",
    "import candidate_adjective_fixer_medium\n",
    "import candidate_adjective_fixer_drug \n",
    "\n",
    "c_dev_cands = session.query(BiomarkerCondition).filter(BiomarkerCondition.split == 1).all()\n",
    "m_dev_cands = session.query(BiomarkerMedium).filter(BiomarkerMedium.split == 1).all()\n",
    "d_dev_cands = session.query(BiomarkerDrug).filter(BiomarkerDrug.split == 1).all()\n",
    "\n",
    "print len(c_dev_cands)\n",
    "print len(m_dev_cands)\n",
    "print len(d_dev_cands)\n",
    "\n",
    "candidate_adjective_fixer_drug.add_adj_candidate(session, BiomarkerDrug, d_dev_cands)\n",
    "candidate_adjective_fixer_medium.add_adj_candidate(session, BiomarkerMedium, m_dev_cands)\n",
    "candidate_adjective_fixer.add_adj_candidate(session, BiomarkerCondition, c_dev_cands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print len(dev_cands)\n",
    "print session.query(BiomarkerCondition).filter(BiomarkerCondition.split == 1).count()\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#from snorkel.lf_terms import *\n",
    "from snorkel.lf_helpers import  *\n",
    "from snorkel.lf_helpers import get_sent_candidate_spans\n",
    "from snorkel.lf_helpers import get_left_tokens, get_right_tokens\n",
    "from random import randint\n",
    "import cPickle\n",
    "from PyDictionary import PyDictionary\n",
    "\n",
    "\n",
    "\n",
    "#umls_dict              = load_umls_dictionary()\n",
    "#chemicals              = load_chemdner_dictionary()\n",
    "#abbrv2text, text2abbrv = load_specialist_abbreviations()\n",
    "\n",
    "keyWords = [\"associate\", \"express\", \"marker\", \"biomarker\", \"elevated\", \"decreased\",\n",
    "            \"correlation\", \"correlates\", \"found\", \"diagnose\", \"variant\", \"appear\",\n",
    "            \"connect\", \"relate\", \"exhibit\", \"indicate\", \"signify\", \"show\", \"demonstrate\",\n",
    "            \"reveal\", \"suggest\", \"evidence\", \"elevation\", \"indication\", \"diagnosis\",\n",
    "            \"variation\", \"modification\", \"suggestion\", \"link\", \"derivation\", \"denote\",\n",
    "            \"denotation\", \"demonstration\", \"magnification\", \"depression\", \"boost\", \"level\",\n",
    "            \"advance\", \"augmentation\", \"lessening\", \"enhancement\", \"expression\", \"buildup\",\n",
    "            \"diminishing\", \"diminishment\", \"reduction\", \"drop\", \"dwindling\", \"lowering\"]\n",
    "\n",
    "negationWords = [\"not\", \"nor\", \"neither\"]\n",
    "\n",
    "toAdd = []\n",
    "for keyword in keyWords:\n",
    "    syns = (PyDictionary().synonym(keyword))\n",
    "    if not syns == None:\n",
    "        for syn in syns:\n",
    "            if not syn in keyWords and not syn in toAdd:\n",
    "                toAdd.append(syn)\n",
    "for word in toAdd:\n",
    "    keyWords.append(word)\n",
    "\n",
    "markerDatabase = []\n",
    "with open('databases/markerData.pickle', 'rb') as f:\n",
    "    markerDatabase = cPickle.load(f)\n",
    "\n",
    "    \n",
    "knowAbbreviations = []\n",
    "with open('databases/abbreviations.com.pkl', 'rb') as f:\n",
    "    knowAbbreviations = cPickle.load(f)\n",
    "    \n",
    "\n",
    "# Biomarker Validity \n",
    "\n",
    "def LF_markerDatabase(c):\n",
    "    if(c.biomarker.get_span() in markerDatabase):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def LF_abstract_titleWord(c):\n",
    "    words_in_between = []\n",
    "    for thing in get_between_tokens(c):\n",
    "        words_in_between.append(thing)\n",
    "    if(len(words_in_between) > 1 and words_in_between[0] == \":\"):\n",
    "        return -1\n",
    "\n",
    "def LF_single_letter(c):\n",
    "    if(len(c.biomarker.get_span()) < 2):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def LF_known_abs(c):\n",
    "    if(c.biomarker.get_span() in knowAbbreviations):\n",
    "        return -1\n",
    "    \n",
    "def LF_same_thing(c):\n",
    "    if(c[0].get_span() == c[1].get_span()):\n",
    "        return -1\n",
    "    \n",
    "def LF_roman_numeral(c):\n",
    "    biomarker = (c.biomarker.get_span())\n",
    "    unicodedata.normalize('NFKD', biomarker).encode('ascii','ignore')\n",
    "    if re.match(r'((?<=\\s)|(?<=^))(M{1,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})|M{0,4}(CM|CD|D?C{1,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})|M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{1,3})(IX|IV|V?I{0,3})|M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{1,3}))(?=\\s)',\n",
    "                biomarker):\n",
    "        return -1 \n",
    "    \n",
    "    \n",
    "# Disease Specific\n",
    "    \n",
    "def LF_distance(c):\n",
    "    '''<TYPE> cancer'''\n",
    "    x = 0\n",
    "    for thing in (get_between_tokens(c)):\n",
    "        x+=1\n",
    "    if(x > 8):\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "    '''if(len(get_between_tokens(c)) < 8):\n",
    "        return 1'''\n",
    "\n",
    "def LF_keyword(c):\n",
    "    for keyword in keyWords:\n",
    "#         print keyword\n",
    "        if(keyword in get_between_tokens(c)):\n",
    "            if(\"not\" in get_between_tokens(c)):\n",
    "                return -1\n",
    "            else:\n",
    "                return 1\n",
    "    return 0\n",
    "    \n",
    "def LF_auxpass(c):\n",
    "    if not 'auxpass' in get_between_tokens(c, attrib='dep_labels'):\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "with open('databases/common2000.pkl', 'rb') as f:\n",
    "    common2000 = cPickle.load(f)\n",
    "\n",
    "def LF_common_2000(c):\n",
    "    if(c.condition.get_span() in common2000):\n",
    "        return -1\n",
    "\n",
    "# Medium Specific\n",
    "# Type Specific\n",
    "# Drug Specific\n",
    "# Level/Units Specific\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BiomarkerSpecificLFs = [LF_markerDatabase, LF_abstract_titleWord, LF_single_letter, LF_known_abs, LF_same_thing, LF_roman_numeral]\n",
    "DiseaseSpecificLFs = [LF_distance,  LF_keyword, LF_auxpass, LF_common_2000]\n",
    "MediumSpecificLFs = []\n",
    "TypeSpecificLFs = []\n",
    "DrugSpecificLFs = []\n",
    "LevelUnitSpecificLFs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "labeler = LabelAnnotator(lfs=LFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time L_train = labeler.apply(split=0)\n",
    "L_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_train.lf_stats(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning.structure import DependencySelector\n",
    "ds = DependencySelector()\n",
    "deps = ds.select(L_train, threshold=0.1)\n",
    "len(deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deps = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel(lf_propensity=True)\n",
    "gen_model.train(\n",
    "    L_train, deps=deps, decay=0.95, step_size=0.1/L_train.shape[0], reg_param=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_model.learned_lf_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import save_marginals\n",
    "save_marginals(session, L_train, train_marginals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from load_external_annotations_new import load_external_labels\n",
    "load_external_labels(session, BiomarkerCondition, 'Biomarker', 'Condition', 'articles/disease_gold_labels.tsv', dev_cands, annotator_name='gold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_dev\n",
    "print L_gold_dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for thing in L_gold_dev:\n",
    "    print thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_dev = labeler.apply_existing(split=1)\n",
    "_ = gen_model.score(session, L_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for candidate in dev_cands:\n",
    "    print candidate[1].get_stable_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L_dev.lf_stats(session, L_gold_dev, gen_model.learned_lf_stats()['Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "labeled = []\n",
    "for c in session.query(BiomarkerCondition).filter(BiomarkerCondition.split == 1).all():\n",
    "    if LF_markerDatabase(c) == 1:\n",
    "        labeled.append(c)\n",
    "SentenceNgramViewer(labeled, session, n_per_page=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load dev labels and convert to [0, 1] range\n",
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "dev_labels = (np.ravel(L_gold_dev.todense()) + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureAnnotator\n",
    "featurizer = FeatureAnnotator()\n",
    "\n",
    "%time F_train = featurizer.apply(split=0)\n",
    "F_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "F_dev  = featurizer.apply_existing(split=1)\n",
    "F_test = featurizer.apply_existing(split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_marginals\n",
    "train_marginals = load_marginals(session, F_train, split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "disc_model = SparseLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning.utils import MentionScorer\n",
    "from snorkel.learning import RandomSearch, ListParameter, RangeParameter\n",
    "\n",
    "# Searching over learning rate\n",
    "rate_param = RangeParameter('lr', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l1_param  = RangeParameter('l1_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l2_param  = RangeParameter('l2_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "\n",
    "searcher = RandomSearch(session, disc_model, F_train, train_marginals, [rate_param, l1_param, l2_param], n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_dev\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1701)\n",
    "searcher.fit(F_dev, L_gold_dev, n_epochs=50, rebalance=0.5, print_freq=25)\n",
    "\n",
    "### Scoring on the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_cands = session.query(BiomarkerCondition).filter(BiomarkerCondition.split == 2).all()\n",
    "train_cands = session.query(BiomarkerCondition).filter(BiomarkerCondition.split == 0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from load_external_annotations_new import load_external_labels\n",
    "load_external_labels(session, BiomarkerCondition, 'Biomarker', 'Condition', 'articles/disease_test_labels.tsv', test_cands, annotator_name='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "L_gold_test\n",
    "\n",
    "tp, fp, tn, fn = disc_model.score(session, F_test, L_gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = disc_model.score(session, F_test, L_gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.rnn import reRNN\n",
    "\n",
    "train_kwargs = {\n",
    "    'lr':         0.01,\n",
    "    'dim':        100,\n",
    "    'n_epochs':   50,\n",
    "    'dropout':    0.5,\n",
    "    'rebalance':  0.25,\n",
    "    'print_freq': 5\n",
    "}\n",
    "\n",
    "lstm = reRNN(seed=1701, n_threads=None)\n",
    "lstm.train(train_cands, train_marginals, dev_candidates=dev_cands, dev_labels=dev_labels, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm.save(\"biomarkercondition.lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
