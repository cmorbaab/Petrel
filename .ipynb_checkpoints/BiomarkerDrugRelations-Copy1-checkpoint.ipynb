{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "import os\n",
    "from snorkel.parser import XMLMultiDocPreprocessor\n",
    "\n",
    "# The following line is for testing only. Feel free to ignore it.\n",
    "file_path = 'articles/training.xml'\n",
    "train_preprocessor = XMLMultiDocPreprocessor(\n",
    "    path=file_path,\n",
    "    doc='.//article',\n",
    "    text='.//front/article-meta/abstract/p/text()',\n",
    "    id=  './/front/article-meta/article-id/text()'\n",
    ")\n",
    "\n",
    "file_path = 'articles/development.xml'\n",
    "dev_preprocessor = XMLMultiDocPreprocessor(\n",
    "    path=file_path,    \n",
    "    doc='.//document',    \n",
    "    text='.//passage/text/text()',    \n",
    "    id='.//id/text()'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.parser import CorpusParser\n",
    "\n",
    "corpus_parser = CorpusParser()\n",
    "corpus_parser.apply(list(train_preprocessor)) #parallelism can be run with a Postgres DBMS, but not SQLite\n",
    "corpus_parser.apply(list(dev_preprocessor), clear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves.cPickle import load\n",
    "\n",
    "with open('articles/doc_ids.pkl', 'rb') as f:\n",
    "    train_ids, dev_ids = load(f)\n",
    "train_ids, dev_ids = set(train_ids), set(dev_ids)\n",
    "print len(train_ids)\n",
    "print len(dev_ids)\n",
    "train_sents, dev_sents = set(), set()\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "print docs\n",
    "print len(docs)\n",
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if doc.name in train_ids:\n",
    "            train_sents.add(s)\n",
    "        elif doc.name in dev_ids:\n",
    "            dev_sents.add(s)\n",
    "        else:\n",
    "            raise Exception('ID <{0}> not found in any id set'.format(doc.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.models import Candidate, candidate_subclass\n",
    "\n",
    "BiomarkerDrug = candidate_subclass('BiomarkerDrug', ['biomarker', 'drug'])\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import PersonMatcher\n",
    "import matchers\n",
    "from snorkel.models import Document\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "import os\n",
    "\n",
    "biomarker_ngrams = Ngrams(n_max=1)\n",
    "drug_ngrams = Ngrams(n_max=5)\n",
    "\n",
    "# Create our two Matchers\n",
    "bMatcher = matchers.getBiomarkerMatcher()\n",
    "dMatcher = matchers.getDrugMatcher()\n",
    "    \n",
    "# Building the CandidateExtractor \n",
    "candidate_extractor = CandidateExtractor(BiomarkerDrug, [biomarker_ngrams, drug_ngrams], [bMatcher, dMatcher])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, sents in enumerate([train_sents, dev_sents]):\n",
    "    candidate_extractor.apply(sents, split=k)\n",
    "    print(\"Number of candidates:\", session.query(BiomarkerDrug).filter(BiomarkerDrug.split == k).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from load_external_annotations_new import load_external_labels\n",
    "load_external_labels(session, BiomarkerDrug, 'Biomarker', 'Drug', 'articles/drug_gold_labels.tsv', annotator_name='gold')\n",
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# load our list of training candidates\n",
    "train_cands = session.query(Candidate).filter(Candidate.split == 0).all()\n",
    "dev_cands = session.query(Candidate).filter(Candidate.split == 1).all()\n",
    "\n",
    "SentenceNgramViewer(train_cands[0:500], session, n_per_page=1) #train_cands = list of cands we want to look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keyWords = ['basis', 'target', 'treat', 'treatment', 'inhibit', 'inhibition', 'inhibitor', ]\n",
    "negationWords = [\"not\", \"nor\", \"neither\"]\n",
    "\n",
    "def presenceOfNot(m):\n",
    "    for word in negationWords:\n",
    "        if (word in m.post_window1('lemmas', 20)) and (word in m.pre_window2('lemmas', 20)):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 1\n",
    "def LF_distance(m):\n",
    "    # if 'neuroendocrine' in m.lemmas:\n",
    "    #     print m.lemmas \n",
    "    # print m.dep_labels\n",
    "    distance = abs(m.e2_idxs[0] - m.e1_idxs[0])\n",
    "    count = 0\n",
    "    for lemma in m.lemmas:\n",
    "        if lemma == ',':\n",
    "            count += 1\n",
    "    if count > 1 and ',' in m.pre_window1('lemmas', 1):\n",
    "        print m\n",
    "        return 0\n",
    "    if distance == 0:\n",
    "        return -1\n",
    "    if distance < 8:\n",
    "        # print \"RETURNING ONE\"\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def LF_keyword(m):\n",
    "    for word in keyWords:\n",
    "        if ((word in m.post_window1('lemmas', 20)) and (word in m.pre_window2('lemmas', 20))) or ((word in m.post_window1('lemmas', 20)) and (word in m.pre_window2('lemmas', 20))):\n",
    "            if presenceOfNot(m):\n",
    "                return -1\n",
    "            else:\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "def LF_usedToTreat(m):\n",
    "     if ('used' in m.pre_window1('lemmas'), 20) and ('to' in m.pre_window1('lemmas', 20)) and ('treat' in m.pre_window1('lemmas', 20)):\n",
    "         return 1\n",
    "     else:\n",
    "         return 0\n",
    "     \n",
    "def LF_usedToInhibit(m):\n",
    "     if ('used' in m.pre_window1('lemmas'), 20) and ('to' in m.pre_window1('lemmas', 20)) and ('inhibit' in m.pre_window1('lemmas', 20)):\n",
    "         return 1\n",
    "     else:\n",
    "         return 0\n",
    "     \n",
    "def LF_inhibit(m):\n",
    "    if ('inhibit' in m.pre_window1('lemmas', 20) and 'inhibit' in m.pre_window2('lemms', 20)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def LF_target(m):\n",
    "    if ('target' in m.pre_window1('lemmas', 20) and 'target' in m.pre_window2('lemms', 20)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def LF_inhibitionOfWith(m):\n",
    "    if ('inhibition' in m.pre_window1('lemmas', 20) and 'of' in m.pre_window1('lemmas', 20) and 'with' in m.post_window1('lemmas', 20)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def LF_isATreatmentOf(m):\n",
    "    if ('is' in m.pre_window1('lemmas', 20) and 'a' in m.pre_window1('lemmas', 20) and 'treatment' in m.pre_window1('lemmas', 20) and 'of' in m.pre_window1('lemmas', 20)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def LF_antibody(m):\n",
    "    if ('antibody' in m.post_window2('lemmas', 20)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def LF_hadisbad(m):\n",
    "    if(m.mention1 == \"had\" or m.mention2 == \"had\"):\n",
    "        return -1\n",
    "def LF_duration(m):\n",
    "    if(m.mention1 == \"duration\" or m.mention2 == \"duration\"):\n",
    "        return -1\n",
    "def LF_isaBiomarker(m):\n",
    "    post_window1_lemmas = m.post_window1('lemmas',20)\n",
    "    pre_window2_lemmas = m.pre_window2('lemmas',20)\n",
    "    if ('biomarker' in post_window1_lemmas and 'biomarker' in pre_window2_lemmas) or ('marker' in post_window1_lemmas and 'marker' in pre_window2_lemmas) or ('indicator' in post_window1_lemmas and 'indicator' in pre_window2_lemmas):\n",
    "        marker_idx_post_window1 = -1\n",
    "        markers = ['biomarker','marker','indicator']\n",
    "        for marker in markers:\n",
    "            try:\n",
    "                # print post_window1_lemmas\n",
    "                findMarker = post_window1_lemmas.index(marker)\n",
    "                if not findMarker == -1:\n",
    "                    marker_idx_post_window1 = findMarker\n",
    "                    print marker\n",
    "            except:\n",
    "                pass\n",
    "        if 'cop' in m.post_window1('dep_labels',20):\n",
    "            try:\n",
    "                cop_idx_post_window1 = m.post_window1('dep_labels',20).index('cop')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            print \"MarkerIdx:\"\n",
    "            print marker_idx_post_window1\n",
    "            print \"ROOTIdx:\"\n",
    "            try:\n",
    "                print  m.post_window1('dep_labels',marker_idx_post_window1)\n",
    "                print  m.post_window1('dep_labels',marker_idx_post_window1).index('ROOT')\n",
    "            except:\n",
    "                pass\n",
    "            print '\\n'\n",
    "            \n",
    "            return 1 if ('nsubj' in m.mention1(attribute='dep_labels')) and (marker_idx_post_window1-cop_idx_post_window1 < 4)  else 0\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LFs = [LF_distance, LF_keyword, LF_usedToTreat, LF_inhibit, LF_inhibitionOFWith, LF_isaBiomarker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "labeler = LabelAnnotator(lfs=LFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1701)\n",
    "\n",
    "%time L_train = labeler.apply(split=0, lfs=LFs, parallelism=4)\n",
    "print L_train.shape\n",
    "\n",
    "%time L_dev = labeler.apply_existing(split=1, lfs=LFs, parallelism=4)\n",
    "print L_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_train.lf_stats(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_dev.lf_stats(session, labels=L_gold_dev.toarray().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib.scoring import *\n",
    "\n",
    "majority_vote_score(L_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "from snorkel.learning import RandomSearch, ListParameter, RangeParameter\n",
    "\n",
    "# use grid search to optimize the generative model\n",
    "step_size_param     = ListParameter('step_size', [0.1 / L_train.shape[0], 1e-5])\n",
    "decay_param         = ListParameter('decay', [0.9, 0.95])\n",
    "epochs_param        = ListParameter('epochs', [50])\n",
    "\n",
    "# search for the best model\n",
    "param_grid = [step_size_param, decay_param, epochs_param]\n",
    "searcher = RandomSearch(GenerativeModel, param_grid, L_train, n=4, lf_propensity=False)\n",
    "%time gen_model, run_stats = searcher.fit(L_dev, L_gold_dev, deps=set())\n",
    "\n",
    "run_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_dev.lf_stats(session, L_gold_dev, gen_model.learned_lf_stats()['Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20, range=(0.0, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_marginals = gen_model.marginals(L_dev)\n",
    "_, _, _, _ = gen_model.error_analysis(session, L_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import save_marginals\n",
    "%time save_marginals(session, L_train, train_marginals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning.structure import DependencySelector\n",
    "ds = DependencySelector()\n",
    "deps = ds.select(L_train, threshold=0.3) # <-- can play with threshold\n",
    "print len(deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning.disc_models.rnn import reRNN\n",
    "\n",
    "train_kwargs = {\n",
    "    'lr':         0.001,\n",
    "    'dim':        100,\n",
    "    'n_epochs':   10,\n",
    "    'dropout':    0.5,\n",
    "    'print_freq': 1,\n",
    "    'batch_size': 128,\n",
    "    'max_sentence_length': 100\n",
    "}\n",
    "\n",
    "# n_epochs = 20?\n",
    "\n",
    "lstm = reRNN(seed=1701, n_threads=7)\n",
    "lstm.train(train_cands, train_marginals, X_dev=dev_cands, Y_dev=L_gold_dev, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p, r, f1 = lstm.score(test_cands, L_gold_test)\n",
    "print(\"Prec: {0:.3f}, Recall: {1:.3f}, F1 Score: {2:.3f}\".format(p, r, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = lstm.error_analysis(session, test_cands, L_gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm.save(\"biomarkercondition.lstm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
